https://github.com/vgaurav3011/Statistics-for-Machine-Learning/blob/master/notebooks/001-Measures-of-Central-Tendency.ipynb

**Module 3: Mathematical Foundations for Machine Learning**

**Linear Algebra: **
Vectors, Matrices, Norms, Subspaces, Projections, SVD, EVD, Derivatives of Matrices, Vector Derivative Identities, and Least Squares


**Optimisation: **
Constrained and Unconstrained Optimisation, Maxima and Minima, Convex and Non-Convex, Gradient and Hessian, Positive Definite and Semi-Definite, Second Derivative Test, Steepest Descent, Adam, AdaGrad, RMSProp, and KKT


**Probability Theory: **
Discrete and Continuous Random Variables, Conditional Probability, Joint Probability Distribution, Multivariate, MAP Criterion, and ML Criterion


**Learning Outcomes**

Gaining an understanding of the mathematical fundamentals crucial for machine and deep learning success, like linear algebra, probability theory, and optimisation methods.

In linear algebra, master essential operations involving vectors and matrices and the understanding of eigenvalues and eigenvectors.

Probability theory will provide concepts on probability distributions and Bayes' theorem, which is crucial to understanding the probabilistic nature of machine learning algorithms.

Furthermore, it delves into optimisation techniques, including gradient descent and convex optimisation, empowering to optimise models and algorithms effectively.

